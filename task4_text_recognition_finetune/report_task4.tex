\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Task 4: TrOCR Fine-Tuning for Persian Receipt Text Recognition}
\author{SHL OCR Team}
\date{24 October 2025}

\begin{document}
\maketitle

\section{Background and Objectives}
The fourth task in the SHL OCR workstream focuses on adapting an attention-based decoder for receipt text recognition. Earlier phases delivered detection and classical recognition baselines; here, we aim to ((i)) construct a reproducible fine-tuning pipeline for the TrOCR-small Vision-Encoder-Decoder (V-former + GPT-2 style decoder), ((ii)) justify the modeling choices and loss setup, ((iii)) benchmark the system against strong PaddleOCR baselines, and ((iv)) provide actionable error analyses and next steps. The dataset of choice is the publicly available \texttt{hezarai/parsynth-ocr-200k} split, which offers diverse Persian synthetic receipts with paired ground-truth strings.

\section{Pipeline and Modeling Rationale}
\paragraph{Preprocessing.} We normalize label text by stripping Tatweel characters, mapping Arabic glyphs to Persian forms, standardizing numerals, and collapsing whitespace. This matches downstream evaluation expectations and keeps training targets within a compact charset (max length 64 tokens).

\paragraph{Tokenizer Extension.} Character coverage is critical for autoregressive decoders. We extract the training-set charset (up to the sample limit) and add missing characters to the TrOCR tokenizer. Only new, previously unknown symbols are added, then the decoder embeddings are resized accordingly.

\paragraph{Data Collation.} Vision inputs are processed via the DeiT image processor bundled with TrOCR. Labels use a custom collator that pads with the tokenizer pad id and applies the loss mask of \(-100\). This mirrors the expected cross-entropy loss behaviour in sequence-to-sequence training.

\paragraph{Training Configuration.} We rely on \texttt{Seq2SeqTrainer} with teacher forcing and standard cross-entropy loss on the decoder tokens. Optional flags allow (a) freezing the vision encoder to accelerate short diagnostic runs and (b) mixed-precision (fp16) for faster throughput. Logging/evaluation is performed on held-out subsets, and generation metrics (CER, WER, exact match) are computed by decoding predictions with the same normalization routine.

\paragraph{Why TrOCR?} Compared to CTC-based recognizers, TrOCR leverages an attention decoder that can model long-range contextual dependencies across text lines, making it resilient against irregular layouts. Its modular vision encoder accommodates transfer learning via synthetic receipts, while the decoder quickly adapts to language-specific character distributions through tokenizer augmentation.

\section{Experiments and Metrics}
\paragraph{Baseline.} PaddleOCR results, evaluated with the shared scripts, establish a solid starting point (Table~\ref{tab:metrics}).

\paragraph{Fine-Tuning Runs.} Two diagnostic TrOCR fine-tunes were executed on time-budget-friendly subsets:
\begin{itemize}[nosep]
  \item \textbf{Run V2} -- 4,000 train / 400 eval samples, encoder frozen, 1 epoch, fp16. Outcome: training completed in \textasciitilde 14 minutes, but overfitting and vocabulary under-coverage yielded near-random metrics (CER $\approx 1.00$, WER $\approx 1.10$ on a 500-sample test slice). Qualitative inspection shows English uppercase hallucinations where Persian glyphs were expected, highlighting the need for deeper adaptation or unfreezing the encoder.
  \item \textbf{Run V3} -- 2,000 train / 200 eval samples, encoder trainable, 3 epochs, fp16. Runtime \textasciitilde 22 minutes with stable loss descent to $\sim 2.76$. Evaluation on the test subset is queued; checkpoints are ready under \texttt{artifacts/trocr\_small\_quick\_v3}. Based on training curves, we expect improved CER relative to Run V2 but have yet to verify quantitatively.
\end{itemize}

\paragraph{Metrics Table.} Table~\ref{tab:metrics} summarises the currently available numbers.

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Model / Run & CER & WER & Exact Match \\
    \midrule
    PaddleOCR Baseline & 0.448 & 0.738 & 0.302 \\
    TrOCR Run V2 (500 test subset) & 0.999 & 1.104 & 0.000 \\
    TrOCR Run V3 & \multicolumn{3}{c}{Pending evaluation} \\
    \bottomrule
  \end{tabular}
  \caption{Character error rate (CER), word error rate (WER), and exact-match accuracy for baseline and quick TrOCR fine-tunes.}
  \label{tab:metrics}
\end{table}

\paragraph{Error Analysis.} Run V2 predictions demonstrate classic partial transfer failure: outputs drift toward Latin uppercase tokens and short numeric fragments (e.g., \texttt{FACEBOOK}, \texttt{27\%}) despite Persian references. This indicates insufficient adaptation time and limited decoder vocabulary coverage, exacerbated by freezing the vision backbone. The synthetic dataset contains many stylistic variants; freezing the encoder prevents alignment updates needed for unseen glyph patterns, explaining the high CER despite low training loss.

\section{Recommendations and Next Steps}
\paragraph{Immediate Actions.} (i) Evaluate Run V3 on 500--1,000 test samples to obtain realistic CER/WER improvements; (ii) deploy the same script on a larger 20k-sample train split with encoder unfrozen to balance runtime and quality; (iii) experiment with \texttt{num\_beams=4} decoding during evaluation to leverage sequence-level scoring (already enabled in the evaluation script).

\paragraph{Model Improvements.} Prioritise tokenizer inspection to guarantee all Persian punctuation and diacritics are covered. Consider curriculum training: start with the synthetic dataset, then fine-tune on any real receipts available. For stability, apply gradient clipping (e.g., norm 1.0) and adjust learning rate schedule (cosine decay) to prevent late-epoch spikes noted in Run V3 logs.

\paragraph{Documentation and Reproducibility.} Artifacts (charsets, checkpoints, metrics JSON, prediction JSONL) are stored under \texttt{nasr/ocr/task4\_text\_recognition\_finetune/artifacts}. The shell commands recorded in this document produce replicable results on a single Tesla V100 (32 GB) GPU. Future reports should append full metric tables once Run V3 evaluation completes and integrate qualitative heatmaps for attention heads to visualise reading order adherence.

\paragraph{Longer-Term Vision.} Extend the pipeline to accept domain-mixed datasets (synthetic + real) with dynamic weighting, and explore multilingual decoders conditioned on BCP-47 tags. This will facilitate generalisation to Task 5 (multi-lingual receipts) while keeping the attention-based architecture consistent.

\end{document}
